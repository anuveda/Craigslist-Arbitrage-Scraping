# -*- coding: utf-8 -*-
"""AnuragIndivProjectDDROfficial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18mJvkfI5rZjR5_l_LEMP1YfhFslOhrJ_

## Anurag Vedagiri - Individual Project 1


# Part 1: Scraping and Saving HTML Content

### 1. Identify the Target

url1 = "https://sfbay.craigslist.org/search/zip?sort=dateoldesth=1~gallery~0~0"

### 2. Interact with the Page-Sorting

- I can trigger the sorting change directly by modifying the URL in the browser's address bar by adding "oldest" in "sort=dateoldest#search".
- Without specifying "oldest",the url would display items from newest down.

- The type of request that is made when I change the sort order is a **GET request**.
- This is because adding "oldest" doesn't necessarily update any data, but it does slightly change the URL in the address bar.
- Since the part of the URL that changes pertains to sorting, I believe for that reason it is a **GET request**.

- I believe that the variable in the URL associated with sorting is simply - "sort".
- If we wanted to change how the webpage is sorted, we would adjust what the "sort=" value would be equal to.

### 3. Interact with the Page-Pagination:

url2 = "https://sfbay.craigslist.org/search/zip?sort=date#search=1~gallery~1~0"

- The part of the URL that changes as I navigate through different pages is at the end of the URL, "gallery~0~0", specifically the 1st number.

- The variable is the 1st number after "gallery~" as it seems to control which page number the craigslist site displays.
- In order to move between pages by only changing the URL, I can simply input a a number other than 0 in the first digit of [gallery~"0"~0].
    - For example, if I was to change that "0~0" to "1~0", that would take me to the 2nd page.
    - "2~0" would take me to the 3rd page, etc.

### 4. Fetch Listing URLs
"""

from bs4 import BeautifulSoup
import requests
import time

headers = {'User-Agent' : 'Mozilla/5.0'}

url = 'https://sfbay.craigslist.org/search/zip#search=1~gallery~0~0'
page = requests.get(url, headers)
soup = BeautifulSoup(page.content, 'html.parser')

all_urls = [look_a['href'] for look_a in soup.select('li.cl-static-search-result a[href]')]

time.sleep(5)

for urls in all_urls[:250]:
    print(urls)

"""#### Explanation
- The selector I chose to grab the link was the 'li.cl-static-search-result a[href]'
- Another possible selection method to retrieve the link to the individual listing could be soup.find_all()
    - It serves a similar purpose in finding all the tags ('li' for example) and look for the href attribute inside
- My strategy was to look for 'li' elements that have an 'a' and a 'href' tag in the free section of the Craiglist website
    - By looking for 'li' elements, this heled me narrow down the specific listings that were posted
    - By looking for 'a' elements, this highlighted exactly where the hyperlinks of the items wre located
    - By looking for 'href' attributes, it helped me make a list of all these specific hyperlinks, which I could then print by using a soup.select
    - At the end, I narrowed down my approach to the top 250 listings by using '[:250]' after 'print(all_urls)'

### 5. Save HTML Pages
"""

import os

my_urls = all_urls[:250]
def save_html_files(urls, directory):

    os.makedirs("craigslist_free_listings", exist_ok = True)
    header = {'User-Agent' : 'Mozilla/5.0'}

    for url in my_urls:
        id = url.split('/')[-1]
        response = requests.get(url, headers = header)
        pathway = os.path.join("craigslist_free_listings", id)
        with open(pathway, 'w', encoding='utf-8') as file:
            file.write(response.text)
            print(f"{id}")

save_html_files(my_urls, "craigslist_free_listings")
time.sleep(5)

"""- I saved each listing iD for each item listed on the craigslist website as a file titled: "771-------".html

# 2. Parsing and Displaying Information from Saved HTML

### 1. Read Saved HTML Files

### 2. Extract Information:
"""

#files_in_directory = os.listdir(directory)
#print(files_in_directory)
"I read each of the saved HTML files and then printed all the files in the directory titled 'craigslist_free_listings' to see if all of my files are correctly saved to the directory."

directory = "craigslist_free_listings"

for filename in os.listdir(directory):
    if filename.endswith(".html"):
        filepath = os.path.join(directory, filename)

        with open(filepath, 'r', encoding='utf-8') as file:
            html = file.read()
            soup = BeautifulSoup(html, 'html.parser')

            all_titles = soup.find('span', id = 'titletextonly')
            listing_title = all_titles.get_text(strip=True) if all_titles else 'None'

            #print(listing_title)

            img = soup.find('img').get('src') if soup.find('img') else 'None'

            #print(img)

            desc = soup.find('section', id = 'postingbody')
            description = desc.get_text(strip=True) if desc else 'None'

            #print(description)

            posts = soup.select_one('div.postinginfos > p.postinginfo')
            postID = posts.get_text(strip=True) if posts else 'None'

            #print(postID)

            post = soup.find('time', class_ = 'date timeago')
            post_Date = post.get_text(strip=True) if post else 'None'

            #print(post_Date)

            updated = soup.select_one('p.postinginfo.reveal:contains("updated") > time.timeago')

            last_updated_date = updated.get_text(strip=True) if updated else 'None'

            #print(last_updated_date)

            print(f"Title: {listing_title}")
            print(f"URL of first image: {img}")
            print(f"Description: {description}")
            print(f"{postID}")
            print(f"Posted Date: {post_Date}")
            print(f"Last Updated Date: {last_updated_date}")
            print("\n")

time.sleep(5)

"""# 3. Automating Login on The Old Reader

### 1. Creating and Verifying a The Old Reader Account

- Username/Email: anuved19@gmail.com
- Password: mypassword19

### 2. Exploring the Login Mechanism

##### Documenting all `<input>` fields within the login form
"""

#1. '<input name="utf8" type = "hidden" value="✓">'
#2. '<input name="authenticity_token" type="hidden" value="GacsNtQPN/rBB4k58q/s9ag/xRpGhEazExbp/2COHIY">'
#3. '<input autocapitalize="off" autocorrect="off" autofocus="autofocus" class="form-control" id="user_login" name="user[login]" placeholder="Username/Email" size="30" spellcheck="false" type="text"> == $0''
#4. '<input class="btn btn-primary btn-block" name="commit" type="submit" value="Sign In">'

"""#### All Input fields are documented above ^^^

### 3. Analyzing Network Traffic for Login Request

- The network request made when I submit the login form is a POST request. This method was chosen because in the login process, it does not change any information on the URL, different from the GET request.
    - This new information gets sent to TheOldReader.com website and usernames and passwords are stored under payload.
- The payload section in comparison to the `<form>` / `<input>` fields that I previously analyzed now stores that info in well-defined rows titled utf8 with a ✓, authenticity token matching the value in the `<form>` / `<input>` fields, my user[login] containing my email, my user[password] containing my password, and lastly, a commit titled 'Sign In'
    - Each `<input>` field has a attribute titled 'name' that matches the info on the payload, as well as 'value' that matches the corresponding value in the payload.

### 4. Automating the Login Process

### 5. Verifying Successful Login
"""

import re

headers = {'User-Agent' : 'Mozilla/5.0'}

url = 'https://theoldreader.com/users/sign_in'
page = requests.get(url, headers)
soup = BeautifulSoup(page.content, 'html.parser')

#print(soup.prettify())
form = soup.select_one('#new_user')

input = form.select_one('input[name=authenticity_token]')
authenticity_token = input.get('value')

#print(authenticity_token)
session = requests.session()

session.post('https://theoldreader.com/users/sign_in',
             data = {'utf8' : '✓',
                     'authenticity_token' : authenticity_token,
                     'user[login]' : 'anuved19@gmail.com',
                     'user[password]' : 'mypassword19',
                     'commit' : 'Sign In'},
                timeout=20)

cookies = session.cookies.get_dict()

url2 = 'https://theoldreader.com/posts/all'
page2 = session.get(url2, cookies = cookies)
soup2 = BeautifulSoup(page2.content, 'html.parser')
#print(soup2.prettify())

time.sleep(5)
login_info = ''.join(name_email.string
for name_email in soup2.find_all('script') if name_email.string and "Bugsnag.user" in name_email.string)
name = re.search(r'name: "(.*?)"', login_info).group(1)
email = re.search(r'email: "(.*?)"', login_info).group(1)
print(f"Login info:")
print(f"name: '{name}'")
print(f"Email: '{email}'")
print(f"Cookies: {cookies}")

